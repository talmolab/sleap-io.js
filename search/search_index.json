{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#sleap-iojs","title":"sleap-io.js","text":"<p>JavaScript/TypeScript utilities for reading and writing SLEAP <code>.slp</code> files with streaming-friendly access patterns and a lightweight data model.</p>"},{"location":"#quick-start","title":"Quick start","text":"<pre><code>npm install\nnpm run build\n</code></pre> <pre><code>import { loadSlp, saveSlp } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"/path/to/session.slp\", { openVideos: false });\nawait saveSlp(labels, \"/tmp/session-roundtrip.slp\", { embed: false });\n</code></pre>"},{"location":"#why-this-project","title":"Why this project","text":"<ul> <li>Bring SLP parsing to browser and serverless environments.</li> <li>Keep large file workflows streaming-first.</li> <li>Mirror the sleap-io data model and codec behaviors in JS.</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>SLP read/write with embedded frame support.</li> <li>Streaming inputs (URL, <code>File</code>, <code>FileSystemFileHandle</code>).</li> <li>Data model types (<code>Labels</code>, <code>LabeledFrame</code>, <code>Instance</code>, <code>Skeleton</code>, <code>Video</code>).</li> <li>Dictionary and numpy codecs.</li> </ul>"},{"location":"#environments","title":"Environments","text":"<ul> <li>Static SPA: Browser-only usage with URL streaming or File System Access API.</li> <li>Server/Worker: Stream from URLs or byte buffers for containerized runtimes.</li> <li>Local Node/Electron: Optional local filesystem access for <code>.slp</code> paths.</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Python sleap-io: https://github.com/talmolab/sleap-io</li> <li>Docs: https://io.sleap.ai</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>This document covers the public API exported from <code>src/index.ts</code>. All examples assume: <pre><code>import {\n  loadSlp,\n  saveSlp,\n  loadVideo,\n  Video,\n  Mp4BoxVideoBackend,\n  Labels,\n  LabeledFrame,\n  Instance,\n  PredictedInstance,\n  Skeleton,\n  Track,\n  LabelsSet,\n  SuggestionFrame,\n  toDict,\n  fromDict,\n  toNumpy,\n  fromNumpy,\n} from \"@talmolab/sleap-io.js\";\n</code></pre></p>"},{"location":"api/#core-io","title":"Core I/O","text":""},{"location":"api/#loadslpsource-options","title":"<code>loadSlp(source, options)</code>","text":"<p>Read <code>.slp</code> from a path, URL, <code>File</code>, <code>FileSystemFileHandle</code>, or byte buffer.</p> <pre><code>const labels = await loadSlp(\"/data/session.slp\", {\n  openVideos: false,\n  h5: { stream: \"auto\", filenameHint: \"session.slp\" },\n});\n</code></pre> <ul> <li><code>source</code>: string path/URL, <code>File</code>, <code>FileSystemFileHandle</code>, <code>Uint8Array</code>, or <code>ArrayBuffer</code>.</li> <li><code>options.openVideos</code> (default <code>true</code>): set <code>false</code> to skip opening video backends.</li> <li><code>options.h5.stream</code>: <code>\"auto\" | \"range\" | \"download\"</code> (browser URL streaming).</li> <li><code>options.h5.filenameHint</code>: helps name temporary files.</li> </ul>"},{"location":"api/#saveslplabels-filename-options","title":"<code>saveSlp(labels, filename, options)</code>","text":"<p>Write <code>.slp</code> (Node/Electron).</p> <pre><code>await saveSlp(labels, \"/tmp/roundtrip.slp\", {\n  embed: false,\n  restoreOriginalVideos: true,\n});\n</code></pre> <ul> <li><code>options.embed</code>: embed frames (<code>true</code>, <code>false</code>, or dataset name).</li> <li><code>options.restoreOriginalVideos</code> (default <code>true</code>): keep original video paths.</li> </ul>"},{"location":"api/#loadvideofilename-options","title":"<code>loadVideo(filename, options)</code>","text":"<p>Open a <code>Video</code> with an appropriate backend.</p> <pre><code>const video = await loadVideo(\"/data/movie.mp4\", { openBackend: true });\nconst frame0 = await video.getFrame(0);\nvideo.close();\n</code></pre> <ul> <li><code>options.dataset</code>: dataset path for embedded HDF5 sources.</li> <li><code>options.openBackend</code> (default <code>true</code>): set <code>false</code> for deferred open.</li> </ul>"},{"location":"api/#video-backends","title":"Video Backends","text":""},{"location":"api/#video","title":"<code>Video</code>","text":"<p><code>Video</code> wraps a backend and exposes <code>shape</code>, <code>fps</code>, <code>getFrame</code>, <code>getFrameTimes</code>, and <code>close</code>.</p> <pre><code>const video = await loadVideo(\"/data/movie.mp4\");\nconst timestamps = await video.getFrameTimes(); // number[] | null\n</code></pre> <ul> <li><code>getFrameTimes()</code> returns <code>null</code> if the backend does not implement it.</li> </ul>"},{"location":"api/#mp4boxvideobackend","title":"<code>Mp4BoxVideoBackend</code>","text":"<p>Browser-only backend for <code>.mp4</code> with WebCodecs + mp4box. Supports range requests when the server honors <code>Range</code> and falls back to full download.</p> <pre><code>const backend = new Mp4BoxVideoBackend(\"/data/movie.mp4\");\nconst video = new Video({ filename: \"/data/movie.mp4\", backend });\nconst t = await video.getFrameTimes();\n</code></pre> <p>Notes: - Requires WebCodecs (<code>VideoDecoder</code>, <code>EncodedVideoChunk</code>). - Uses HEAD + <code>Range: bytes=0-0</code> probe to detect range support. - Provides precise frame timestamps via <code>getFrameTimes()</code>.</p>"},{"location":"api/#hdf5videobackend","title":"<code>Hdf5VideoBackend</code>","text":"<p>Used for <code>.slp</code>, <code>.h5</code>, or <code>.hdf5</code> sources (embedded frames).</p> <ul> <li>Selected automatically by <code>loadVideo()</code> for HDF5 inputs.</li> <li>Decodes embedded PNG/JPEG frames in browsers; raw frames require <code>shape</code> + <code>channelOrder</code>.</li> </ul>"},{"location":"api/#mediavideobackend","title":"<code>MediaVideoBackend</code>","text":"<p>Browser fallback when WebCodecs/mp4box is unavailable.</p> <ul> <li>Uses <code>HTMLVideoElement</code> + canvas.</li> <li>No <code>getFrameTimes()</code> support.</li> </ul>"},{"location":"api/#labels-model-classes","title":"Labels &amp; Model Classes","text":"<p>Key types:</p> <ul> <li><code>Labels</code>, <code>LabeledFrame</code>, <code>Instance</code>, <code>PredictedInstance</code></li> <li><code>Skeleton</code>, <code>Track</code></li> <li><code>Video</code>, <code>SuggestionFrame</code>, <code>LabelsSet</code></li> <li><code>Camera</code>, <code>CameraGroup</code>, <code>RecordingSession</code> (camera utilities)</li> </ul> <pre><code>const skeleton = new Skeleton([\"nose\", \"tail\"]);\nconst inst = Instance.fromArray([[10, 20], [30, 40]], skeleton);\nconst video = new Video({ filename: \"/data/movie.mp4\" });\nconst frame = new LabeledFrame({ video, frameIdx: 0, instances: [inst] });\nconst labels = new Labels({ labeledFrames: [frame], skeletons: [skeleton], videos: [video] });\n</code></pre>"},{"location":"api/#codecs-numpy-helpers","title":"Codecs &amp; Numpy Helpers","text":""},{"location":"api/#dictionary-codec","title":"Dictionary codec","text":"<pre><code>const dict = toDict(labels);\nconst restored = fromDict(dict);\n</code></pre>"},{"location":"api/#numpy-codec","title":"Numpy codec","text":"<pre><code>const array = toNumpy(labels, { returnConfidence: true });\nconst rebuilt = fromNumpy(array, { video, skeleton, returnConfidence: true });\n</code></pre> <p>Also available: - <code>Labels.numpy({ video, returnConfidence })</code> - <code>Labels.fromNumpy(data, { video, skeleton, trackNames, firstFrame })</code></p>"},{"location":"api/#streaming-options","title":"Streaming Options","text":"<p>SLP streaming is handled by HDF5 open helpers:</p> <pre><code>const labels = await loadSlp(\"https://example.com/session.slp\", {\n  openVideos: false,\n  h5: { stream: \"range\", filenameHint: \"session.slp\" },\n});\n</code></pre> <ul> <li><code>stream: \"auto\"</code> uses range streaming when available.</li> <li><code>stream: \"download\"</code> forces full file download.</li> <li>Node only supports string paths or byte buffers for SLP inputs.</li> <li>Browser supports URL, <code>File</code>, <code>FileSystemFileHandle</code>, or byte buffers.</li> </ul>"},{"location":"api/#error-handling-notes","title":"Error Handling Notes","text":"<p>Common runtime errors to anticipate:</p> <ul> <li>SLP parsing:</li> <li>Missing <code>/metadata</code> group in invalid <code>.slp</code>.</li> <li>Unsupported source type in browser or Node.</li> <li>Failed fetch for remote SLP (bad URL/status).</li> <li> <p><code>h5wasm</code> FS unavailable in unsupported environments.</p> </li> <li> <p>MP4/WebCodecs:</p> </li> <li>WebCodecs not supported or non-browser environment.</li> <li>No video tracks or unsupported codec.</li> <li> <p>Failed fetch or no video source available.</p> </li> <li> <p>Media backend:</p> </li> <li>Browser-only usage.</li> <li>Video load or seek failures.</li> </ul>"},{"location":"demo/","title":"Demo","text":""},{"location":"demo/#demo","title":"Demo","text":"<p>The embedded viewer below loads <code>.slp</code> files entirely in the browser. It streams the demo SLP and MP4 using range requests when available.</p> <p> </p>"},{"location":"rendering/","title":"Rendering","text":""},{"location":"rendering/#rendering","title":"Rendering","text":"<p>sleap-io.js provides pose visualization using skia-canvas, a high-performance 2D graphics library for Node.js.</p>"},{"location":"rendering/#quick-start","title":"Quick Start","text":"<p>Render a single frame to an image:</p> <pre><code>import { loadSlp, renderImage, saveImage } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nconst imageData = await renderImage(labels.labeledFrames[0], {\n  width: 640,\n  height: 480,\n});\nawait saveImage(imageData, \"output.png\");\n</code></pre> <p>Render a full video (requires ffmpeg):</p> <pre><code>import { loadSlp, renderVideo } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nawait renderVideo(labels, \"output.mp4\");\n</code></pre>"},{"location":"rendering/#color-schemes","title":"Color Schemes","text":"<p>Color scheme determines how poses are colored across instances and frames.</p>"},{"location":"rendering/#color-by-track","title":"Color by track","text":"<p>Each tracked animal gets a consistent color across all frames:</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"track\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-by-instance","title":"Color by instance","text":"<p>Each animal within a frame gets a unique color (colors may change between frames):</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"instance\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-by-node","title":"Color by node","text":"<p>Each body part gets a unique color (same across all animals):</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"node\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#auto-detection","title":"Auto detection","text":"<p>By default (<code>colorBy: \"auto\"</code>), the color scheme is chosen based on context: - If tracks are present \u2192 <code>\"track\"</code> - If single image with no tracks \u2192 <code>\"instance\"</code> - Otherwise \u2192 <code>\"node\"</code> (prevents flicker in videos)</p>"},{"location":"rendering/#color-palettes","title":"Color Palettes","text":""},{"location":"rendering/#built-in-palettes","title":"Built-in palettes","text":"<p>9 palettes are included:</p> Palette Description <code>standard</code> MATLAB default colors (default) <code>distinct</code> High-contrast colors for many instances <code>tableau10</code> Data visualization standard <code>viridis</code> Perceptually uniform scientific <code>rainbow</code> Spectrum colors for node types <code>warm</code> Orange/red tones <code>cool</code> Blue/purple tones <code>pastel</code> Subtle colors for overlays <code>seaborn</code> Professional look for publications <pre><code>const imageData = await renderImage(lf, {\n  colorBy: \"node\",\n  palette: \"tableau10\",\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#getting-palette-colors-programmatically","title":"Getting palette colors programmatically","text":"<pre><code>import { getPalette } from \"@talmolab/sleap-io.js\";\n\nconst colors = getPalette(\"tableau10\", 10);\n// Returns: [[31, 119, 180], [255, 127, 14], ...]\n</code></pre>"},{"location":"rendering/#marker-shapes","title":"Marker Shapes","text":"<p>Five marker shapes are available for node visualization:</p> Shape Description <code>circle</code> Filled circle (default) <code>square</code> Filled square <code>diamond</code> Rotated square <code>triangle</code> Upward-pointing triangle <code>cross</code> Plus sign <pre><code>const imageData = await renderImage(lf, {\n  markerShape: \"diamond\",\n  markerSize: 6,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#styling-options","title":"Styling Options","text":""},{"location":"rendering/#marker-and-line-sizes","title":"Marker and line sizes","text":"<pre><code>// Small markers and thin lines\nconst small = await renderImage(lf, { markerSize: 3, lineWidth: 1.5, width: 640, height: 480 });\n\n// Medium (default)\nconst medium = await renderImage(lf, { markerSize: 4, lineWidth: 2, width: 640, height: 480 });\n\n// Large markers and thick lines\nconst large = await renderImage(lf, { markerSize: 10, lineWidth: 5, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#transparency","title":"Transparency","text":"<pre><code>// Full opacity (default)\nconst opaque = await renderImage(lf, { alpha: 1.0, width: 640, height: 480 });\n\n// Semi-transparent overlay\nconst semi = await renderImage(lf, { alpha: 0.5, width: 640, height: 480 });\n\n// Subtle overlay\nconst subtle = await renderImage(lf, { alpha: 0.25, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#toggle-elements","title":"Toggle elements","text":"<pre><code>// Both nodes and edges (default)\nconst both = await renderImage(lf, { showNodes: true, showEdges: true, width: 640, height: 480 });\n\n// Edges only\nconst edgesOnly = await renderImage(lf, { showNodes: false, showEdges: true, width: 640, height: 480 });\n\n// Nodes only\nconst nodesOnly = await renderImage(lf, { showNodes: true, showEdges: false, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#scaling","title":"Scaling","text":"<p>The <code>scale</code> parameter resizes the output. Graphics (markers, lines) scale proportionally:</p> <pre><code>// Full resolution (default)\nconst full = await renderImage(lf, { scale: 1.0, width: 640, height: 480 });\n\n// Half resolution - faster, smaller files\nconst half = await renderImage(lf, { scale: 0.5, width: 640, height: 480 });\n\n// Double resolution\nconst double = await renderImage(lf, { scale: 2.0, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#background-control","title":"Background Control","text":""},{"location":"rendering/#transparent-background-default","title":"Transparent background (default)","text":"<pre><code>const imageData = await renderImage(lf, {\n  background: \"transparent\",\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#solid-color-background","title":"Solid color background","text":"<pre><code>// Named color\nconst black = await renderImage(lf, { background: \"black\", width: 640, height: 480 });\n\n// RGB tuple\nconst gray = await renderImage(lf, { background: [40, 40, 40], width: 640, height: 480 });\n\n// Hex color\nconst hex = await renderImage(lf, { background: \"#1a1a2e\", width: 640, height: 480 });\n\n// Grayscale\nconst grayscale = await renderImage(lf, { background: 128, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-specification-formats","title":"Color specification formats","text":"<p>The <code>background</code> parameter accepts many formats:</p> Format Example Description Named color <code>\"black\"</code>, <code>\"white\"</code>, <code>\"gray\"</code> Predefined color names Hex (6-digit) <code>\"#ff8000\"</code> Standard hex color Hex (3-digit) <code>\"#f80\"</code> Shorthand hex RGB tuple <code>[255, 128, 0]</code> Values 0-255 Grayscale <code>128</code> Single value 0-255 Palette index <code>\"tableau10[0]\"</code> Color from palette <p>Available named colors: <code>black</code>, <code>white</code>, <code>red</code>, <code>green</code>, <code>blue</code>, <code>yellow</code>, <code>cyan</code>, <code>magenta</code>, <code>gray</code>/<code>grey</code>, <code>orange</code>, <code>purple</code>, <code>pink</code>, <code>brown</code>.</p>"},{"location":"rendering/#custom-rendering-with-callbacks","title":"Custom Rendering with Callbacks","text":"<p>Callbacks let you add custom graphics with direct access to the canvas context.</p> Callback Context Type When Called <code>preRenderCallback</code> <code>RenderContext</code> Before poses are drawn <code>postRenderCallback</code> <code>RenderContext</code> After all poses are drawn <code>perInstanceCallback</code> <code>InstanceContext</code> After each instance is drawn"},{"location":"rendering/#instance-labels","title":"Instance labels","text":"<p>Draw track names above each instance:</p> <pre><code>import { renderImage, InstanceContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawLabels(ctx: InstanceContext): void {\n  const centroid = ctx.getCentroid();\n  if (!centroid) return;\n\n  const [cx, cy] = ctx.worldToCanvas(centroid[0], centroid[1]);\n  const label = ctx.trackName ?? `Instance ${ctx.instanceIdx}`;\n\n  ctx.canvas.font = \"14px Arial\";\n  ctx.canvas.fillStyle = \"rgba(0, 0, 0, 0.6)\";\n  const metrics = ctx.canvas.measureText(label);\n  ctx.canvas.fillRect(cx - 2, cy - 18, metrics.width + 4, 16);\n\n  ctx.canvas.fillStyle = \"white\";\n  ctx.canvas.fillText(label, cx, cy - 6);\n}\n\nconst imageData = await renderImage(lf, {\n  perInstanceCallback: drawLabels,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#bounding-boxes","title":"Bounding boxes","text":"<p>Draw bounding boxes around instances:</p> <pre><code>import { renderImage, InstanceContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawBbox(ctx: InstanceContext): void {\n  const bbox = ctx.getBbox();\n  if (!bbox) return;\n\n  const [x1, y1] = ctx.worldToCanvas(bbox[0], bbox[1]);\n  const [x2, y2] = ctx.worldToCanvas(bbox[2], bbox[3]);\n  const pad = 8;\n\n  ctx.canvas.strokeStyle = \"white\";\n  ctx.canvas.lineWidth = 2;\n  ctx.canvas.setLineDash([6, 3]);\n  ctx.canvas.strokeRect(x1 - pad, y1 - pad, x2 - x1 + 2 * pad, y2 - y1 + 2 * pad);\n  ctx.canvas.setLineDash([]);\n}\n\nconst imageData = await renderImage(lf, {\n  perInstanceCallback: drawBbox,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#frame-info-overlay","title":"Frame info overlay","text":"<p>Add frame number and instance count:</p> <pre><code>import { renderImage, RenderContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawFrameInfo(ctx: RenderContext): void {\n  const text = `Frame: ${ctx.frameIdx}  Instances: ${ctx.instances.length}`;\n\n  ctx.canvas.fillStyle = \"rgba(0, 0, 0, 0.7)\";\n  ctx.canvas.fillRect(4, 4, 200, 20);\n\n  ctx.canvas.font = \"14px Arial\";\n  ctx.canvas.fillStyle = \"white\";\n  ctx.canvas.fillText(text, 8, 18);\n}\n\nconst imageData = await renderImage(lf, {\n  postRenderCallback: drawFrameInfo,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#video-rendering","title":"Video Rendering","text":"<p>Video rendering requires ffmpeg to be installed and in your PATH.</p>"},{"location":"rendering/#basic-video-rendering","title":"Basic video rendering","text":"<pre><code>import { loadSlp, renderVideo } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nawait renderVideo(labels, \"output.mp4\");\n</code></pre>"},{"location":"rendering/#render-a-clip","title":"Render a clip","text":"<pre><code>await renderVideo(labels, \"clip.mp4\", { start: 100, end: 200 });\n</code></pre>"},{"location":"rendering/#specific-frames","title":"Specific frames","text":"<pre><code>await renderVideo(labels, \"selected.mp4\", { frameInds: [0, 50, 100, 150, 200] });\n</code></pre>"},{"location":"rendering/#encoding-options","title":"Encoding options","text":"<pre><code>await renderVideo(labels, \"output.mp4\", {\n  fps: 30,           // Output frame rate\n  crf: 18,           // Quality (lower = better, default: 25)\n  preset: \"slow\",    // Encoding speed (default: \"superfast\")\n  codec: \"libx264\",  // Video codec (default: \"libx264\")\n});\n</code></pre>"},{"location":"rendering/#progress-tracking","title":"Progress tracking","text":"<pre><code>await renderVideo(labels, \"output.mp4\", {\n  onProgress: (current, total) =&gt; {\n    console.log(`Rendering frame ${current}/${total}`);\n  },\n});\n</code></pre>"},{"location":"rendering/#export-utilities","title":"Export Utilities","text":""},{"location":"rendering/#save-to-file","title":"Save to file","text":"<pre><code>import { renderImage, saveImage } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nawait saveImage(imageData, \"output.png\");\nawait saveImage(imageData, \"output.jpg\");\n</code></pre>"},{"location":"rendering/#convert-to-buffer","title":"Convert to buffer","text":"<pre><code>import { renderImage, toPNG, toJPEG } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nconst pngBuffer = await toPNG(imageData);\nconst jpegBuffer = await toJPEG(imageData, 0.9); // quality 0-1\n</code></pre>"},{"location":"rendering/#convert-to-data-url","title":"Convert to data URL","text":"<pre><code>import { renderImage, toDataURL } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nconst dataUrl = toDataURL(imageData, \"png\");\n// \"data:image/png;base64,...\"\n</code></pre>"},{"location":"rendering/#api-reference","title":"API Reference","text":""},{"location":"rendering/#renderimagesource-options","title":"<code>renderImage(source, options)</code>","text":"<p>Render poses from a <code>Labels</code>, <code>LabeledFrame</code>, or array of <code>Instance</code>/<code>PredictedInstance</code>.</p> <p>Parameters: - <code>source</code>: <code>Labels | LabeledFrame | Instance[]</code> - <code>options.width</code>: Frame width (required if no image provided) - <code>options.height</code>: Frame height (required if no image provided) - <code>options.colorBy</code>: <code>\"track\" | \"instance\" | \"node\" | \"auto\"</code> (default: <code>\"auto\"</code>) - <code>options.palette</code>: Palette name (default: <code>\"standard\"</code>) - <code>options.markerShape</code>: <code>\"circle\" | \"square\" | \"diamond\" | \"triangle\" | \"cross\"</code> (default: <code>\"circle\"</code>) - <code>options.markerSize</code>: Marker radius in pixels (default: <code>4</code>) - <code>options.lineWidth</code>: Edge line width (default: <code>2</code>) - <code>options.alpha</code>: Opacity 0-1 (default: <code>1</code>) - <code>options.showNodes</code>: Draw nodes (default: <code>true</code>) - <code>options.showEdges</code>: Draw edges (default: <code>true</code>) - <code>options.scale</code>: Output scale factor (default: <code>1</code>) - <code>options.background</code>: <code>\"transparent\"</code> or color spec - <code>options.image</code>: Background <code>ImageData</code> - <code>options.preRenderCallback</code>: <code>(ctx: RenderContext) =&gt; void</code> - <code>options.postRenderCallback</code>: <code>(ctx: RenderContext) =&gt; void</code> - <code>options.perInstanceCallback</code>: <code>(ctx: InstanceContext) =&gt; void</code></p> <p>Returns: <code>Promise&lt;ImageData&gt;</code></p>"},{"location":"rendering/#rendervideosource-outputpath-options","title":"<code>renderVideo(source, outputPath, options)</code>","text":"<p>Render video with pose overlays. Requires ffmpeg.</p> <p>Parameters: - <code>source</code>: <code>Labels | LabeledFrame[]</code> - <code>outputPath</code>: Output video file path - <code>options</code>: All <code>renderImage</code> options plus:   - <code>options.frameInds</code>: Specific frame indices to render   - <code>options.start</code>: Start frame index   - <code>options.end</code>: End frame index (exclusive)   - <code>options.fps</code>: Output frame rate (default: <code>30</code>)   - <code>options.codec</code>: Video codec (default: <code>\"libx264\"</code>)   - <code>options.crf</code>: Quality factor (default: <code>25</code>)   - <code>options.preset</code>: Encoding preset (default: <code>\"superfast\"</code>)   - <code>options.onProgress</code>: <code>(current: number, total: number) =&gt; void</code></p> <p>Returns: <code>Promise&lt;void&gt;</code></p>"},{"location":"rendering/#rendercontext","title":"<code>RenderContext</code>","text":"<p>Context passed to <code>preRenderCallback</code> and <code>postRenderCallback</code>.</p> <p>Properties: - <code>canvas</code>: <code>CanvasRenderingContext2D</code> - <code>frameIdx</code>: Frame index - <code>frameSize</code>: <code>[width, height]</code> - <code>instances</code>: Array of instances - <code>skeletonEdges</code>: <code>[srcIdx, dstIdx][]</code> - <code>nodeNames</code>: <code>string[]</code> - <code>scale</code>: Current scale factor - <code>offset</code>: <code>[x, y]</code> offset</p> <p>Methods: - <code>worldToCanvas(x, y)</code>: Transform world coordinates to canvas coordinates</p>"},{"location":"rendering/#instancecontext","title":"<code>InstanceContext</code>","text":"<p>Context passed to <code>perInstanceCallback</code>.</p> <p>Properties: - <code>canvas</code>: <code>CanvasRenderingContext2D</code> - <code>instanceIdx</code>: Instance index within frame - <code>points</code>: <code>[[x, y], ...]</code> coordinates - <code>skeletonEdges</code>: <code>[srcIdx, dstIdx][]</code> - <code>nodeNames</code>: <code>string[]</code> - <code>trackIdx</code>: Track index or <code>null</code> - <code>trackName</code>: Track name or <code>null</code> - <code>confidence</code>: Instance score or <code>null</code> - <code>scale</code>: Current scale factor - <code>offset</code>: <code>[x, y]</code> offset</p> <p>Methods: - <code>worldToCanvas(x, y)</code>: Transform world coordinates to canvas coordinates - <code>getCentroid()</code>: Get centroid of valid points, or <code>null</code> - <code>getBbox()</code>: Get <code>[x1, y1, x2, y2]</code> bounding box, or <code>null</code></p>"}]}