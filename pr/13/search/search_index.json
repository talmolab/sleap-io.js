{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#sleap-iojs","title":"sleap-io.js","text":"<p>JavaScript/TypeScript utilities for reading and writing SLEAP <code>.slp</code> files with streaming-friendly access patterns and a lightweight data model.</p>"},{"location":"#quick-start","title":"Quick start","text":"<pre><code>npm install\nnpm run build\n</code></pre> <pre><code>import { loadSlp, saveSlp } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"/path/to/session.slp\", { openVideos: false });\nawait saveSlp(labels, \"/tmp/session-roundtrip.slp\", { embed: false });\n</code></pre>"},{"location":"#why-this-project","title":"Why this project","text":"<ul> <li>Bring SLP parsing to browser and serverless environments.</li> <li>Keep large file workflows streaming-first.</li> <li>Mirror the sleap-io data model and codec behaviors in JS.</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>SLP read/write with embedded frame support.</li> <li>Streaming inputs (URL, <code>File</code>, <code>FileSystemFileHandle</code>).</li> <li>Data model types (<code>Labels</code>, <code>LabeledFrame</code>, <code>Instance</code>, <code>Skeleton</code>, <code>Video</code>).</li> <li>Dictionary and numpy codecs.</li> <li>Lite mode for Workers-compatible metadata extraction (no WASM).</li> </ul>"},{"location":"#environments","title":"Environments","text":"<ul> <li>Static SPA: Browser-only usage with URL streaming or File System Access API.</li> <li>Server/Worker: Stream from URLs or byte buffers for containerized runtimes.</li> <li>Local Node/Electron: Optional local filesystem access for <code>.slp</code> paths.</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Python sleap-io: https://github.com/talmolab/sleap-io</li> <li>Docs: https://io.sleap.ai</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>This document covers the public API exported from <code>src/index.ts</code>. All examples assume: <pre><code>import {\n  loadSlp,\n  saveSlp,\n  loadVideo,\n  Video,\n  Mp4BoxVideoBackend,\n  Labels,\n  LabeledFrame,\n  Instance,\n  PredictedInstance,\n  Skeleton,\n  Track,\n  LabelsSet,\n  SuggestionFrame,\n  toDict,\n  fromDict,\n  toNumpy,\n  fromNumpy,\n} from \"@talmolab/sleap-io.js\";\n</code></pre></p>"},{"location":"api/#core-io","title":"Core I/O","text":""},{"location":"api/#loadslpsource-options","title":"<code>loadSlp(source, options)</code>","text":"<p>Read <code>.slp</code> from a path, URL, <code>File</code>, <code>FileSystemFileHandle</code>, or byte buffer.</p> <pre><code>const labels = await loadSlp(\"/data/session.slp\", {\n  openVideos: false,\n  h5: { stream: \"auto\", filenameHint: \"session.slp\" },\n});\n</code></pre> <ul> <li><code>source</code>: string path/URL, <code>File</code>, <code>FileSystemFileHandle</code>, <code>Uint8Array</code>, or <code>ArrayBuffer</code>.</li> <li><code>options.openVideos</code> (default <code>true</code>): set <code>false</code> to skip opening video backends.</li> <li><code>options.h5.stream</code>: <code>\"auto\" | \"range\" | \"download\"</code> (browser URL streaming).</li> <li><code>options.h5.filenameHint</code>: helps name temporary files.</li> </ul>"},{"location":"api/#saveslplabels-filename-options","title":"<code>saveSlp(labels, filename, options)</code>","text":"<p>Write <code>.slp</code> (Node/Electron).</p> <pre><code>await saveSlp(labels, \"/tmp/roundtrip.slp\", {\n  embed: false,\n  restoreOriginalVideos: true,\n});\n</code></pre> <ul> <li><code>options.embed</code>: embed frames (<code>true</code>, <code>false</code>, or dataset name).</li> <li><code>options.restoreOriginalVideos</code> (default <code>true</code>): keep original video paths.</li> </ul>"},{"location":"api/#loadvideofilename-options","title":"<code>loadVideo(filename, options)</code>","text":"<p>Open a <code>Video</code> with an appropriate backend.</p> <pre><code>const video = await loadVideo(\"/data/movie.mp4\", { openBackend: true });\nconst frame0 = await video.getFrame(0);\nvideo.close();\n</code></pre> <ul> <li><code>options.dataset</code>: dataset path for embedded HDF5 sources.</li> <li><code>options.openBackend</code> (default <code>true</code>): set <code>false</code> for deferred open.</li> </ul>"},{"location":"api/#video-backends","title":"Video Backends","text":""},{"location":"api/#video","title":"<code>Video</code>","text":"<p><code>Video</code> wraps a backend and exposes <code>shape</code>, <code>fps</code>, <code>getFrame</code>, <code>getFrameTimes</code>, and <code>close</code>.</p> <pre><code>const video = await loadVideo(\"/data/movie.mp4\");\nconst timestamps = await video.getFrameTimes(); // number[] | null\n</code></pre> <ul> <li><code>getFrameTimes()</code> returns <code>null</code> if the backend does not implement it.</li> </ul>"},{"location":"api/#mp4boxvideobackend","title":"<code>Mp4BoxVideoBackend</code>","text":"<p>Browser-only backend for <code>.mp4</code> with WebCodecs + mp4box. Supports range requests when the server honors <code>Range</code> and falls back to full download.</p> <pre><code>const backend = new Mp4BoxVideoBackend(\"/data/movie.mp4\");\nconst video = new Video({ filename: \"/data/movie.mp4\", backend });\nconst t = await video.getFrameTimes();\n</code></pre> <p>Notes: - Requires WebCodecs (<code>VideoDecoder</code>, <code>EncodedVideoChunk</code>). - Uses HEAD + <code>Range: bytes=0-0</code> probe to detect range support. - Provides precise frame timestamps via <code>getFrameTimes()</code>.</p>"},{"location":"api/#hdf5videobackend","title":"<code>Hdf5VideoBackend</code>","text":"<p>Used for <code>.slp</code>, <code>.h5</code>, or <code>.hdf5</code> sources (embedded frames).</p> <ul> <li>Selected automatically by <code>loadVideo()</code> for HDF5 inputs.</li> <li>Decodes embedded PNG/JPEG frames in browsers; raw frames require <code>shape</code> + <code>channelOrder</code>.</li> </ul>"},{"location":"api/#mediavideobackend","title":"<code>MediaVideoBackend</code>","text":"<p>Browser fallback when WebCodecs/mp4box is unavailable.</p> <ul> <li>Uses <code>HTMLVideoElement</code> + canvas.</li> <li>No <code>getFrameTimes()</code> support.</li> </ul>"},{"location":"api/#labels-model-classes","title":"Labels &amp; Model Classes","text":"<p>Key types:</p> <ul> <li><code>Labels</code>, <code>LabeledFrame</code>, <code>Instance</code>, <code>PredictedInstance</code></li> <li><code>Skeleton</code>, <code>Track</code></li> <li><code>Video</code>, <code>SuggestionFrame</code>, <code>LabelsSet</code></li> <li><code>Camera</code>, <code>CameraGroup</code>, <code>RecordingSession</code> (camera utilities)</li> </ul> <pre><code>const skeleton = new Skeleton([\"nose\", \"tail\"]);\nconst inst = Instance.fromArray([[10, 20], [30, 40]], skeleton);\nconst video = new Video({ filename: \"/data/movie.mp4\" });\nconst frame = new LabeledFrame({ video, frameIdx: 0, instances: [inst] });\nconst labels = new Labels({ labeledFrames: [frame], skeletons: [skeleton], videos: [video] });\n</code></pre>"},{"location":"api/#codecs-numpy-helpers","title":"Codecs &amp; Numpy Helpers","text":""},{"location":"api/#dictionary-codec","title":"Dictionary codec","text":"<pre><code>const dict = toDict(labels);\nconst restored = fromDict(dict);\n</code></pre>"},{"location":"api/#numpy-codec","title":"Numpy codec","text":"<pre><code>const array = toNumpy(labels, { returnConfidence: true });\nconst rebuilt = fromNumpy(array, { video, skeleton, returnConfidence: true });\n</code></pre> <p>Also available: - <code>Labels.numpy({ video, returnConfidence })</code> - <code>Labels.fromNumpy(data, { video, skeleton, trackNames, firstFrame })</code></p>"},{"location":"api/#streaming-options","title":"Streaming Options","text":"<p>SLP streaming is handled by HDF5 open helpers:</p> <pre><code>const labels = await loadSlp(\"https://example.com/session.slp\", {\n  openVideos: false,\n  h5: { stream: \"range\", filenameHint: \"session.slp\" },\n});\n</code></pre> <ul> <li><code>stream: \"auto\"</code> uses range streaming when available.</li> <li><code>stream: \"download\"</code> forces full file download.</li> <li>Node only supports string paths or byte buffers for SLP inputs.</li> <li>Browser supports URL, <code>File</code>, <code>FileSystemFileHandle</code>, or byte buffers.</li> </ul>"},{"location":"api/#error-handling-notes","title":"Error Handling Notes","text":"<p>Common runtime errors to anticipate:</p> <ul> <li>SLP parsing:</li> <li>Missing <code>/metadata</code> group in invalid <code>.slp</code>.</li> <li>Unsupported source type in browser or Node.</li> <li>Failed fetch for remote SLP (bad URL/status).</li> <li> <p><code>h5wasm</code> FS unavailable in unsupported environments.</p> </li> <li> <p>MP4/WebCodecs:</p> </li> <li>WebCodecs not supported or non-browser environment.</li> <li>No video tracks or unsupported codec.</li> <li> <p>Failed fetch or no video source available.</p> </li> <li> <p>Media backend:</p> </li> <li>Browser-only usage.</li> <li>Video load or seek failures.</li> </ul>"},{"location":"demo/","title":"Demo","text":""},{"location":"demo/#demo","title":"Demo","text":"<p>The embedded viewer below loads <code>.slp</code> files entirely in the browser. It streams the demo SLP and MP4 using range requests when available.</p> <p> </p>"},{"location":"lite/","title":"Lite Mode","text":""},{"location":"lite/#lite-mode","title":"Lite Mode","text":"<p>The <code>/lite</code> entry point provides a Workers-compatible SLP reader that uses jsfive (pure JavaScript) instead of h5wasm (WebAssembly).</p>"},{"location":"lite/#when-to-use-lite-mode","title":"When to use Lite Mode","text":"<p>Use the lite module when:</p> <ul> <li>Running in Cloudflare Workers or other environments that block runtime WebAssembly compilation</li> <li>You only need metadata (skeletons, counts, video info) without actual pose coordinates</li> <li>You want a smaller bundle (~50KB vs ~800KB+ for h5wasm)</li> <li>You need quick validation of SLP files</li> </ul>"},{"location":"lite/#installation","title":"Installation","text":"<pre><code>import { loadSlpMetadata, validateSlpBuffer, isHdf5Buffer } from \"@talmolab/sleap-io.js/lite\";\n</code></pre>"},{"location":"lite/#api-reference","title":"API Reference","text":""},{"location":"lite/#loadslpmetadatasource-options","title":"<code>loadSlpMetadata(source, options?)</code>","text":"<p>Extract metadata from an SLP file without loading pose coordinates.</p> <pre><code>const response = await fetch(\"https://example.com/file.slp\");\nconst buffer = await response.arrayBuffer();\nconst metadata = await loadSlpMetadata(buffer);\n\nconsole.log(metadata.version);      // \"1.3.4\"\nconsole.log(metadata.skeletons);    // Full skeleton definitions\nconsole.log(metadata.tracks);       // Track names\nconsole.log(metadata.videos);       // Video metadata\nconsole.log(metadata.counts);       // { labeledFrames, instances, points, predictedPoints }\nconsole.log(metadata.provenance);   // { sleap_version, ... }\n</code></pre> <p>Parameters:</p> <ul> <li><code>source</code>: <code>ArrayBuffer</code> or <code>Uint8Array</code> containing the SLP file</li> <li><code>options.filename</code>: Optional filename hint for embedded video paths</li> </ul> <p>Returns: <code>Promise&lt;SlpMetadata&gt;</code></p>"},{"location":"lite/#validateslpbuffersource","title":"<code>validateSlpBuffer(source)</code>","text":"<p>Quick structural validation of an SLP file.</p> <pre><code>try {\n  validateSlpBuffer(buffer);\n  console.log(\"Valid SLP file\");\n} catch (e) {\n  console.error(\"Invalid:\", e.message);\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>source</code>: <code>ArrayBuffer</code> or <code>Uint8Array</code> to validate</li> </ul> <p>Returns: <code>true</code> if valid, throws <code>Error</code> with details if invalid</p>"},{"location":"lite/#ishdf5buffersource","title":"<code>isHdf5Buffer(source)</code>","text":"<p>Check if a buffer starts with the HDF5 magic number.</p> <pre><code>if (isHdf5Buffer(buffer)) {\n  // Might be an SLP file, do full validation\n  const metadata = await loadSlpMetadata(buffer);\n}\n</code></pre> <p>Parameters:</p> <ul> <li><code>source</code>: <code>ArrayBuffer</code> or <code>Uint8Array</code> to check</li> </ul> <p>Returns: <code>boolean</code></p>"},{"location":"lite/#types","title":"Types","text":""},{"location":"lite/#slpmetadata","title":"<code>SlpMetadata</code>","text":"<pre><code>interface SlpMetadata {\n  /** SLEAP version that created this file (e.g., \"1.3.4\") */\n  version: string;\n\n  /** HDF5 format ID (e.g., 1.2) */\n  formatId: number;\n\n  /** Skeleton definitions with nodes, edges, and symmetries */\n  skeletons: Skeleton[];\n\n  /** Track definitions */\n  tracks: Track[];\n\n  /** Video metadata (without loaded backends) */\n  videos: VideoMetadata[];\n\n  /** Suggestion frame metadata */\n  suggestions: SuggestionMetadata[];\n\n  /** Multi-camera recording session metadata */\n  sessions: SessionMetadata[];\n\n  /** Dataset counts */\n  counts: {\n    labeledFrames: number;\n    instances: number;\n    points: number;\n    predictedPoints: number;\n  };\n\n  /** Whether any video has embedded image data */\n  hasEmbeddedImages: boolean;\n\n  /** Raw provenance data (SLEAP version, build info, etc.) */\n  provenance?: Record&lt;string, unknown&gt;;\n}\n</code></pre>"},{"location":"lite/#videometadata","title":"<code>VideoMetadata</code>","text":"<pre><code>interface VideoMetadata {\n  filename: string;\n  dataset?: string;\n  format?: string;\n  width?: number;\n  height?: number;\n  channels?: number;\n  fps?: number;\n  frameCount?: number;\n  channelOrder?: string;\n  embedded: boolean;\n  sourceVideo?: { filename: string };\n}\n</code></pre>"},{"location":"lite/#whats-available-vs-not-available","title":"What's Available vs Not Available","text":""},{"location":"lite/#available-in-lite-mode","title":"\u2705 Available in Lite Mode","text":"Metadata Description Skeletons Full definitions with nodes, edges, symmetries Tracks Track names Videos Filename, dimensions, format, fps Counts Frame, instance, and point counts Provenance SLEAP version and build info Sessions Multi-camera session metadata Suggestions Suggestion frame info"},{"location":"lite/#not-available-in-lite-mode","title":"\u274c Not Available in Lite Mode","text":"Data Reason Pose coordinates (x, y) Requires compound dataset support Point visibility/scores Requires compound dataset support Instance-frame mapping Requires compound dataset support Video frame data Requires VLEN sequence support"},{"location":"lite/#full-vs-lite-comparison","title":"Full vs Lite Comparison","text":"Feature <code>loadSlp()</code> <code>loadSlpMetadata()</code> HDF5 backend h5wasm (WASM) jsfive (pure JS) Bundle size ~800KB+ ~50KB Workers compatible \u274c \u2705 Returns <code>Labels</code> object <code>SlpMetadata</code> object Pose coordinates \u2705 \u274c Video frame access \u2705 \u274c Skeleton parsing \u2705 \u2705 Metadata \u2705 \u2705"},{"location":"lite/#example-file-upload-validation","title":"Example: File Upload Validation","text":"<pre><code>import { loadSlpMetadata, validateSlpBuffer, isHdf5Buffer } from \"@talmolab/sleap-io.js/lite\";\n\nasync function handleUpload(file: File) {\n  const buffer = await file.arrayBuffer();\n\n  // Quick check\n  if (!isHdf5Buffer(buffer)) {\n    throw new Error(\"Not an HDF5 file\");\n  }\n\n  // Validate structure\n  validateSlpBuffer(buffer);\n\n  // Extract metadata\n  const metadata = await loadSlpMetadata(buffer, { filename: file.name });\n\n  return {\n    valid: true,\n    skeletons: metadata.skeletons.map(s =&gt; s.name),\n    frameCount: metadata.counts.labeledFrames,\n    instanceCount: metadata.counts.instances,\n    sleapVersion: metadata.provenance?.sleap_version,\n  };\n}\n</code></pre>"},{"location":"rendering/","title":"Rendering","text":""},{"location":"rendering/#rendering","title":"Rendering","text":"<p>sleap-io.js provides pose visualization using skia-canvas, a high-performance 2D graphics library for Node.js.</p>"},{"location":"rendering/#quick-start","title":"Quick Start","text":"<p>Render a single frame to an image:</p> <pre><code>import { loadSlp, renderImage, saveImage } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nconst imageData = await renderImage(labels.labeledFrames[0], {\n  width: 640,\n  height: 480,\n});\nawait saveImage(imageData, \"output.png\");\n</code></pre> <p>Render a full video (requires ffmpeg):</p> <pre><code>import { loadSlp, renderVideo } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nawait renderVideo(labels, \"output.mp4\");\n</code></pre>"},{"location":"rendering/#color-schemes","title":"Color Schemes","text":"<p>Color scheme determines how poses are colored across instances and frames.</p>"},{"location":"rendering/#color-by-track","title":"Color by track","text":"<p>Each tracked animal gets a consistent color across all frames:</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"track\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-by-instance","title":"Color by instance","text":"<p>Each animal within a frame gets a unique color (colors may change between frames):</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"instance\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-by-node","title":"Color by node","text":"<p>Each body part gets a unique color (same across all animals):</p> <pre><code>const imageData = await renderImage(lf, { colorBy: \"node\", width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#auto-detection","title":"Auto detection","text":"<p>By default (<code>colorBy: \"auto\"</code>), the color scheme is chosen based on context: - If tracks are present \u2192 <code>\"track\"</code> - If single image with no tracks \u2192 <code>\"instance\"</code> - Otherwise \u2192 <code>\"node\"</code> (prevents flicker in videos)</p>"},{"location":"rendering/#color-palettes","title":"Color Palettes","text":""},{"location":"rendering/#built-in-palettes","title":"Built-in palettes","text":"<p>9 palettes are included:</p> Palette Description <code>standard</code> MATLAB default colors (default) <code>distinct</code> High-contrast colors for many instances <code>tableau10</code> Data visualization standard <code>viridis</code> Perceptually uniform scientific <code>rainbow</code> Spectrum colors for node types <code>warm</code> Orange/red tones <code>cool</code> Blue/purple tones <code>pastel</code> Subtle colors for overlays <code>seaborn</code> Professional look for publications <pre><code>const imageData = await renderImage(lf, {\n  colorBy: \"node\",\n  palette: \"tableau10\",\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#getting-palette-colors-programmatically","title":"Getting palette colors programmatically","text":"<pre><code>import { getPalette } from \"@talmolab/sleap-io.js\";\n\nconst colors = getPalette(\"tableau10\", 10);\n// Returns: [[31, 119, 180], [255, 127, 14], ...]\n</code></pre>"},{"location":"rendering/#marker-shapes","title":"Marker Shapes","text":"<p>Five marker shapes are available for node visualization:</p> Shape Description <code>circle</code> Filled circle (default) <code>square</code> Filled square <code>diamond</code> Rotated square <code>triangle</code> Upward-pointing triangle <code>cross</code> Plus sign <pre><code>const imageData = await renderImage(lf, {\n  markerShape: \"diamond\",\n  markerSize: 6,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#styling-options","title":"Styling Options","text":""},{"location":"rendering/#marker-and-line-sizes","title":"Marker and line sizes","text":"<pre><code>// Small markers and thin lines\nconst small = await renderImage(lf, { markerSize: 3, lineWidth: 1.5, width: 640, height: 480 });\n\n// Medium (default)\nconst medium = await renderImage(lf, { markerSize: 4, lineWidth: 2, width: 640, height: 480 });\n\n// Large markers and thick lines\nconst large = await renderImage(lf, { markerSize: 10, lineWidth: 5, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#transparency","title":"Transparency","text":"<pre><code>// Full opacity (default)\nconst opaque = await renderImage(lf, { alpha: 1.0, width: 640, height: 480 });\n\n// Semi-transparent overlay\nconst semi = await renderImage(lf, { alpha: 0.5, width: 640, height: 480 });\n\n// Subtle overlay\nconst subtle = await renderImage(lf, { alpha: 0.25, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#toggle-elements","title":"Toggle elements","text":"<pre><code>// Both nodes and edges (default)\nconst both = await renderImage(lf, { showNodes: true, showEdges: true, width: 640, height: 480 });\n\n// Edges only\nconst edgesOnly = await renderImage(lf, { showNodes: false, showEdges: true, width: 640, height: 480 });\n\n// Nodes only\nconst nodesOnly = await renderImage(lf, { showNodes: true, showEdges: false, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#scaling","title":"Scaling","text":"<p>The <code>scale</code> parameter resizes the output. Graphics (markers, lines) scale proportionally:</p> <pre><code>// Full resolution (default)\nconst full = await renderImage(lf, { scale: 1.0, width: 640, height: 480 });\n\n// Half resolution - faster, smaller files\nconst half = await renderImage(lf, { scale: 0.5, width: 640, height: 480 });\n\n// Double resolution\nconst double = await renderImage(lf, { scale: 2.0, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#background-control","title":"Background Control","text":""},{"location":"rendering/#transparent-background-default","title":"Transparent background (default)","text":"<pre><code>const imageData = await renderImage(lf, {\n  background: \"transparent\",\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#solid-color-background","title":"Solid color background","text":"<pre><code>// Named color\nconst black = await renderImage(lf, { background: \"black\", width: 640, height: 480 });\n\n// RGB tuple\nconst gray = await renderImage(lf, { background: [40, 40, 40], width: 640, height: 480 });\n\n// Hex color\nconst hex = await renderImage(lf, { background: \"#1a1a2e\", width: 640, height: 480 });\n\n// Grayscale\nconst grayscale = await renderImage(lf, { background: 128, width: 640, height: 480 });\n</code></pre>"},{"location":"rendering/#color-specification-formats","title":"Color specification formats","text":"<p>The <code>background</code> parameter accepts many formats:</p> Format Example Description Named color <code>\"black\"</code>, <code>\"white\"</code>, <code>\"gray\"</code> Predefined color names Hex (6-digit) <code>\"#ff8000\"</code> Standard hex color Hex (3-digit) <code>\"#f80\"</code> Shorthand hex RGB tuple <code>[255, 128, 0]</code> Values 0-255 Grayscale <code>128</code> Single value 0-255 Palette index <code>\"tableau10[0]\"</code> Color from palette <p>Available named colors: <code>black</code>, <code>white</code>, <code>red</code>, <code>green</code>, <code>blue</code>, <code>yellow</code>, <code>cyan</code>, <code>magenta</code>, <code>gray</code>/<code>grey</code>, <code>orange</code>, <code>purple</code>, <code>pink</code>, <code>brown</code>.</p>"},{"location":"rendering/#custom-rendering-with-callbacks","title":"Custom Rendering with Callbacks","text":"<p>Callbacks let you add custom graphics with direct access to the canvas context.</p> Callback Context Type When Called <code>preRenderCallback</code> <code>RenderContext</code> Before poses are drawn <code>postRenderCallback</code> <code>RenderContext</code> After all poses are drawn <code>perInstanceCallback</code> <code>InstanceContext</code> After each instance is drawn"},{"location":"rendering/#instance-labels","title":"Instance labels","text":"<p>Draw track names above each instance:</p> <pre><code>import { renderImage, InstanceContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawLabels(ctx: InstanceContext): void {\n  const centroid = ctx.getCentroid();\n  if (!centroid) return;\n\n  const [cx, cy] = ctx.worldToCanvas(centroid[0], centroid[1]);\n  const label = ctx.trackName ?? `Instance ${ctx.instanceIdx}`;\n\n  ctx.canvas.font = \"14px Arial\";\n  ctx.canvas.fillStyle = \"rgba(0, 0, 0, 0.6)\";\n  const metrics = ctx.canvas.measureText(label);\n  ctx.canvas.fillRect(cx - 2, cy - 18, metrics.width + 4, 16);\n\n  ctx.canvas.fillStyle = \"white\";\n  ctx.canvas.fillText(label, cx, cy - 6);\n}\n\nconst imageData = await renderImage(lf, {\n  perInstanceCallback: drawLabels,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#bounding-boxes","title":"Bounding boxes","text":"<p>Draw bounding boxes around instances:</p> <pre><code>import { renderImage, InstanceContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawBbox(ctx: InstanceContext): void {\n  const bbox = ctx.getBbox();\n  if (!bbox) return;\n\n  const [x1, y1] = ctx.worldToCanvas(bbox[0], bbox[1]);\n  const [x2, y2] = ctx.worldToCanvas(bbox[2], bbox[3]);\n  const pad = 8;\n\n  ctx.canvas.strokeStyle = \"white\";\n  ctx.canvas.lineWidth = 2;\n  ctx.canvas.setLineDash([6, 3]);\n  ctx.canvas.strokeRect(x1 - pad, y1 - pad, x2 - x1 + 2 * pad, y2 - y1 + 2 * pad);\n  ctx.canvas.setLineDash([]);\n}\n\nconst imageData = await renderImage(lf, {\n  perInstanceCallback: drawBbox,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#frame-info-overlay","title":"Frame info overlay","text":"<p>Add frame number and instance count:</p> <pre><code>import { renderImage, RenderContext } from \"@talmolab/sleap-io.js\";\n\nfunction drawFrameInfo(ctx: RenderContext): void {\n  const text = `Frame: ${ctx.frameIdx}  Instances: ${ctx.instances.length}`;\n\n  ctx.canvas.fillStyle = \"rgba(0, 0, 0, 0.7)\";\n  ctx.canvas.fillRect(4, 4, 200, 20);\n\n  ctx.canvas.font = \"14px Arial\";\n  ctx.canvas.fillStyle = \"white\";\n  ctx.canvas.fillText(text, 8, 18);\n}\n\nconst imageData = await renderImage(lf, {\n  postRenderCallback: drawFrameInfo,\n  width: 640,\n  height: 480,\n});\n</code></pre>"},{"location":"rendering/#video-rendering","title":"Video Rendering","text":"<p>Video rendering requires ffmpeg to be installed and in your PATH.</p>"},{"location":"rendering/#basic-video-rendering","title":"Basic video rendering","text":"<pre><code>import { loadSlp, renderVideo } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"predictions.slp\");\nawait renderVideo(labels, \"output.mp4\");\n</code></pre>"},{"location":"rendering/#render-a-clip","title":"Render a clip","text":"<pre><code>await renderVideo(labels, \"clip.mp4\", { start: 100, end: 200 });\n</code></pre>"},{"location":"rendering/#specific-frames","title":"Specific frames","text":"<pre><code>await renderVideo(labels, \"selected.mp4\", { frameInds: [0, 50, 100, 150, 200] });\n</code></pre>"},{"location":"rendering/#encoding-options","title":"Encoding options","text":"<pre><code>await renderVideo(labels, \"output.mp4\", {\n  fps: 30,           // Output frame rate\n  crf: 18,           // Quality (lower = better, default: 25)\n  preset: \"slow\",    // Encoding speed (default: \"superfast\")\n  codec: \"libx264\",  // Video codec (default: \"libx264\")\n});\n</code></pre>"},{"location":"rendering/#progress-tracking","title":"Progress tracking","text":"<pre><code>await renderVideo(labels, \"output.mp4\", {\n  onProgress: (current, total) =&gt; {\n    console.log(`Rendering frame ${current}/${total}`);\n  },\n});\n</code></pre>"},{"location":"rendering/#export-utilities","title":"Export Utilities","text":""},{"location":"rendering/#save-to-file","title":"Save to file","text":"<pre><code>import { renderImage, saveImage } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nawait saveImage(imageData, \"output.png\");\nawait saveImage(imageData, \"output.jpg\");\n</code></pre>"},{"location":"rendering/#convert-to-buffer","title":"Convert to buffer","text":"<pre><code>import { renderImage, toPNG, toJPEG } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nconst pngBuffer = await toPNG(imageData);\nconst jpegBuffer = await toJPEG(imageData, 0.9); // quality 0-1\n</code></pre>"},{"location":"rendering/#convert-to-data-url","title":"Convert to data URL","text":"<pre><code>import { renderImage, toDataURL } from \"@talmolab/sleap-io.js\";\n\nconst imageData = await renderImage(lf, { width: 640, height: 480 });\nconst dataUrl = toDataURL(imageData, \"png\");\n// \"data:image/png;base64,...\"\n</code></pre>"},{"location":"rendering/#api-reference","title":"API Reference","text":""},{"location":"rendering/#renderimagesource-options","title":"<code>renderImage(source, options)</code>","text":"<p>Render poses from a <code>Labels</code>, <code>LabeledFrame</code>, or array of <code>Instance</code>/<code>PredictedInstance</code>.</p> <p>Parameters: - <code>source</code>: <code>Labels | LabeledFrame | Instance[]</code> - <code>options.width</code>: Frame width (required if no image provided) - <code>options.height</code>: Frame height (required if no image provided) - <code>options.colorBy</code>: <code>\"track\" | \"instance\" | \"node\" | \"auto\"</code> (default: <code>\"auto\"</code>) - <code>options.palette</code>: Palette name (default: <code>\"standard\"</code>) - <code>options.markerShape</code>: <code>\"circle\" | \"square\" | \"diamond\" | \"triangle\" | \"cross\"</code> (default: <code>\"circle\"</code>) - <code>options.markerSize</code>: Marker radius in pixels (default: <code>4</code>) - <code>options.lineWidth</code>: Edge line width (default: <code>2</code>) - <code>options.alpha</code>: Opacity 0-1 (default: <code>1</code>) - <code>options.showNodes</code>: Draw nodes (default: <code>true</code>) - <code>options.showEdges</code>: Draw edges (default: <code>true</code>) - <code>options.scale</code>: Output scale factor (default: <code>1</code>) - <code>options.background</code>: <code>\"transparent\"</code> or color spec - <code>options.image</code>: Background <code>ImageData</code> - <code>options.preRenderCallback</code>: <code>(ctx: RenderContext) =&gt; void</code> - <code>options.postRenderCallback</code>: <code>(ctx: RenderContext) =&gt; void</code> - <code>options.perInstanceCallback</code>: <code>(ctx: InstanceContext) =&gt; void</code></p> <p>Returns: <code>Promise&lt;ImageData&gt;</code></p>"},{"location":"rendering/#rendervideosource-outputpath-options","title":"<code>renderVideo(source, outputPath, options)</code>","text":"<p>Render video with pose overlays. Requires ffmpeg.</p> <p>Parameters: - <code>source</code>: <code>Labels | LabeledFrame[]</code> - <code>outputPath</code>: Output video file path - <code>options</code>: All <code>renderImage</code> options plus:   - <code>options.frameInds</code>: Specific frame indices to render   - <code>options.start</code>: Start frame index   - <code>options.end</code>: End frame index (exclusive)   - <code>options.fps</code>: Output frame rate (default: <code>30</code>)   - <code>options.codec</code>: Video codec (default: <code>\"libx264\"</code>)   - <code>options.crf</code>: Quality factor (default: <code>25</code>)   - <code>options.preset</code>: Encoding preset (default: <code>\"superfast\"</code>)   - <code>options.onProgress</code>: <code>(current: number, total: number) =&gt; void</code></p> <p>Returns: <code>Promise&lt;void&gt;</code></p>"},{"location":"rendering/#rendercontext","title":"<code>RenderContext</code>","text":"<p>Context passed to <code>preRenderCallback</code> and <code>postRenderCallback</code>.</p> <p>Properties: - <code>canvas</code>: <code>CanvasRenderingContext2D</code> - <code>frameIdx</code>: Frame index - <code>frameSize</code>: <code>[width, height]</code> - <code>instances</code>: Array of instances - <code>skeletonEdges</code>: <code>[srcIdx, dstIdx][]</code> - <code>nodeNames</code>: <code>string[]</code> - <code>scale</code>: Current scale factor - <code>offset</code>: <code>[x, y]</code> offset</p> <p>Methods: - <code>worldToCanvas(x, y)</code>: Transform world coordinates to canvas coordinates</p>"},{"location":"rendering/#instancecontext","title":"<code>InstanceContext</code>","text":"<p>Context passed to <code>perInstanceCallback</code>.</p> <p>Properties: - <code>canvas</code>: <code>CanvasRenderingContext2D</code> - <code>instanceIdx</code>: Instance index within frame - <code>points</code>: <code>[[x, y], ...]</code> coordinates - <code>skeletonEdges</code>: <code>[srcIdx, dstIdx][]</code> - <code>nodeNames</code>: <code>string[]</code> - <code>trackIdx</code>: Track index or <code>null</code> - <code>trackName</code>: Track name or <code>null</code> - <code>confidence</code>: Instance score or <code>null</code> - <code>scale</code>: Current scale factor - <code>offset</code>: <code>[x, y]</code> offset</p> <p>Methods: - <code>worldToCanvas(x, y)</code>: Transform world coordinates to canvas coordinates - <code>getCentroid()</code>: Get centroid of valid points, or <code>null</code> - <code>getBbox()</code>: Get <code>[x1, y1, x2, y2]</code> bounding box, or <code>null</code></p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#usage-guide","title":"Usage Guide","text":"<p>This guide covers how to use <code>sleap-io.js</code> in both Node.js and browser environments.</p>"},{"location":"usage/#installation","title":"Installation","text":"<pre><code>npm install @talmolab/sleap-io.js\n</code></pre>"},{"location":"usage/#nodejs-usage","title":"Node.js Usage","text":"<p>In Node.js, all features work out of the box:</p> <pre><code>import { loadSlp, loadVideo } from \"@talmolab/sleap-io.js\";\n\n// Load from file path\nconst labels = await loadSlp(\"path/to/file.slp\");\n\n// Access data\nconsole.log(labels.skeletons[0].nodeNames);\nconsole.log(labels.labeledFrames.length);\n\n// Iterate over frames\nfor (const frame of labels.labeledFrames) {\n  console.log(`Frame ${frame.frameIdx}: ${frame.instances.length} instances`);\n}\n</code></pre>"},{"location":"usage/#loading-videos","title":"Loading Videos","text":"<pre><code>import { loadVideo } from \"@talmolab/sleap-io.js\";\n\nconst video = await loadVideo(\"path/to/video.mp4\");\nconst frame = await video.getFrame(0);  // Returns ImageData or raw bytes\n</code></pre>"},{"location":"usage/#server-side-rendering","title":"Server-Side Rendering","text":"<p>For server-side skeleton rendering (e.g., generating thumbnails):</p> <pre><code>import { renderLabelsImage } from \"@talmolab/sleap-io.js\";\n\nconst labels = await loadSlp(\"file.slp\");\nconst pngBuffer = await renderLabelsImage(labels, {\n  frameIdx: 0,\n  width: 640,\n  height: 480,\n});\n// pngBuffer is a Node.js Buffer containing PNG data\n</code></pre> <p>See rendering.md for more details.</p>"},{"location":"usage/#browser-usage","title":"Browser Usage","text":"<p>Browser usage requires an import map to resolve external dependencies.</p>"},{"location":"usage/#required-import-map","title":"Required Import Map","text":"<pre><code>&lt;script type=\"importmap\"&gt;\n{\n  \"imports\": {\n    \"h5wasm\": \"https://unpkg.com/h5wasm@0.8.8/dist/esm/hdf5_hl.js\",\n    \"yaml\": \"https://esm.sh/yaml@2.6.1\",\n    \"skia-canvas\": \"data:text/javascript,export class Canvas{}\",\n    \"child_process\": \"data:text/javascript,export function spawn(){}\"\n  }\n}\n&lt;/script&gt;\n</code></pre> <p>Dependencies explained:</p> Module Purpose Browser handling <code>h5wasm</code> HDF5 file reading (WebAssembly) Load from CDN <code>yaml</code> YAML skeleton parsing Load from CDN <code>skia-canvas</code> Server-side rendering (Node.js only) Stub out <code>child_process</code> Process spawning (Node.js only) Stub out"},{"location":"usage/#loading-slp-files","title":"Loading SLP Files","text":"<pre><code>&lt;script type=\"module\"&gt;\nimport { loadSlp } from \"./dist/index.js\";\n\n// From URL (uses HTTP Range requests)\nconst labels = await loadSlp(\"https://example.com/file.slp\", {\n  h5: { stream: \"range\" }\n});\n\n// From ArrayBuffer (e.g., file upload)\nconst buffer = await file.arrayBuffer();\nconst labels = await loadSlp(buffer, {\n  h5: { filenameHint: file.name }\n});\n&lt;/script&gt;\n</code></pre>"},{"location":"usage/#external-video-mode","title":"External Video Mode","text":"<p>For SLP files with separate video files:</p> <pre><code>import { loadSlp, loadVideo } from \"./dist/index.js\";\n\n// Load SLP without opening embedded videos\nconst labels = await loadSlp(slpUrl, { openVideos: false });\n\n// Load external video separately\nconst video = await loadVideo(videoUrl);\nconst frame = await video.getFrame(frameIndex);\n</code></pre>"},{"location":"usage/#embedded-images-mode","title":"Embedded Images Mode","text":"<p>For SLP files with embedded images (<code>.pkg.slp</code> files):</p> <pre><code>import { loadSlp } from \"./dist/index.js\";\n\n// Load SLP with embedded video backends\nconst labels = await loadSlp(buffer, { openVideos: true });\n\n// Access frames via video backend\nfor (const frame of labels.labeledFrames) {\n  const video = frame.video;\n  const imageData = await video.backend.getFrame(frame.frameIdx);\n  // imageData is ImageBitmap or ImageData\n}\n</code></pre>"},{"location":"usage/#multi-video-files","title":"Multi-Video Files","text":"<p>Some SLP files contain multiple videos (e.g., validation datasets). Each <code>LabeledFrame</code> references its source video:</p> <pre><code>const labels = await loadSlp(buffer, { openVideos: true });\n\nconsole.log(`${labels.videos.length} videos`);\n\nfor (const frame of labels.labeledFrames) {\n  const videoIndex = labels.videos.indexOf(frame.video);\n  console.log(`Frame from video ${videoIndex}, index ${frame.frameIdx}`);\n\n  // Get embedded image\n  const image = await frame.video.backend?.getFrame(frame.frameIdx);\n}\n</code></pre>"},{"location":"usage/#complete-browser-example","title":"Complete Browser Example","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;script type=\"importmap\"&gt;\n  {\n    \"imports\": {\n      \"h5wasm\": \"https://unpkg.com/h5wasm@0.8.8/dist/esm/hdf5_hl.js\",\n      \"yaml\": \"https://esm.sh/yaml@2.6.1\",\n      \"skia-canvas\": \"data:text/javascript,export class Canvas{}\",\n      \"child_process\": \"data:text/javascript,export function spawn(){}\"\n    }\n  }\n  &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;input type=\"file\" id=\"file\" accept=\".slp\" /&gt;\n  &lt;pre id=\"output\"&gt;&lt;/pre&gt;\n\n  &lt;script type=\"module\"&gt;\n    import { loadSlp } from \"./dist/index.js\";\n\n    document.getElementById(\"file\").addEventListener(\"change\", async (e) =&gt; {\n      const file = e.target.files[0];\n      const buffer = await file.arrayBuffer();\n\n      const labels = await loadSlp(buffer, {\n        openVideos: true,\n        h5: { filenameHint: file.name }\n      });\n\n      const output = [\n        `Skeletons: ${labels.skeletons.length}`,\n        `Videos: ${labels.videos.length}`,\n        `Labeled Frames: ${labels.labeledFrames.length}`,\n        `Tracks: ${labels.tracks.length}`,\n        ``,\n        `Skeleton nodes: ${labels.skeletons[0]?.nodeNames.join(\", \")}`,\n      ];\n\n      document.getElementById(\"output\").textContent = output.join(\"\\\\n\");\n    });\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"usage/#lite-mode-workers-compatible","title":"Lite Mode (Workers-Compatible)","text":"<p>For environments that don't support WebAssembly compilation at runtime (e.g., Cloudflare Workers), use the lite entry point:</p> <pre><code>import { loadSlpMetadata } from \"@talmolab/sleap-io.js/lite\";\n\nconst metadata = await loadSlpMetadata(buffer);\nconsole.log(metadata.skeletons);\nconsole.log(metadata.counts.labeledFrames);\n</code></pre> <p>Lite mode uses pure JavaScript (jsfive) instead of WebAssembly (h5wasm), but only provides metadata - not pose coordinates.</p> <p>See lite.md for full documentation.</p>"},{"location":"usage/#options-reference","title":"Options Reference","text":""},{"location":"usage/#loadslpsource-options","title":"<code>loadSlp(source, options?)</code>","text":"Option Type Default Description <code>openVideos</code> <code>boolean</code> <code>true</code> Open video backends for embedded images <code>h5.stream</code> <code>\"range\"</code> | <code>undefined</code> <code>undefined</code> Use HTTP Range requests for streaming <code>h5.filenameHint</code> <code>string</code> <code>undefined</code> Filename hint for embedded video paths"},{"location":"usage/#loadvideosource","title":"<code>loadVideo(source)</code>","text":"<p>Supports: - File paths (Node.js) - URLs (browser, with CORS) - <code>ArrayBuffer</code> (both)</p> <p>Returns a video backend with: - <code>getFrame(index)</code> - Get frame as <code>ImageBitmap</code>, <code>ImageData</code>, or raw bytes - <code>getFrameTimes()</code> - Get array of frame timestamps - <code>fps</code> - Frames per second - <code>shape</code> - <code>[frames, height, width, channels]</code></p>"},{"location":"usage/#error-handling","title":"Error Handling","text":"<pre><code>try {\n  const labels = await loadSlp(source);\n} catch (error) {\n  if (error.message.includes(\"Missing /metadata\")) {\n    console.error(\"Invalid SLP file: not a SLEAP labels file\");\n  } else if (error.message.includes(\"fetch\")) {\n    console.error(\"Network error loading file\");\n  } else {\n    console.error(\"Unknown error:\", error);\n  }\n}\n</code></pre>"},{"location":"usage/#typescript-support","title":"TypeScript Support","text":"<p>Full TypeScript definitions are included:</p> <pre><code>import type { Labels, LabeledFrame, Instance, Skeleton } from \"@talmolab/sleap-io.js\";\n\nconst labels: Labels = await loadSlp(source);\nconst frame: LabeledFrame = labels.labeledFrames[0];\nconst instance: Instance = frame.instances[0];\nconst skeleton: Skeleton = instance.skeleton;\n</code></pre>"}]}